# System Design Visualization
# Generated: 2026-02-15
# THIS FILE IS FOR LOCAL REVIEW ONLY -- DO NOT COMMIT
# It is listed in .gitignore to prevent accidental commits.

## 1. System Overview

This project is a reproduction and enhancement of the paper "AI-Driven Clinical Decision
Support: Enhancing Disease Diagnosis Exploiting Patients Similarity" (Comito et al., 2022,
IEEE Access). It implements a patient similarity-based disease diagnosis prediction system
using NLP embeddings over MIMIC-III clinical data.

The system has TWO parallel pipelines:
1. **Baseline Pipeline** -- BioSentVec (Sent2Vec, 700D) sentence embeddings
2. **BERT Enhancement Pipeline** -- Clinical BERT models (768D) from HuggingFace

Both pipelines share identical preprocessing, evaluation metrics, data splits, and output
format, enabling direct comparison.

### Key Facts
- **Dataset**: 128 patient admissions from MIMIC-III (Symptoms-Diagnosis.txt)
- **Evaluation**: 10-fold cross-validation (~116 train / ~13 test per fold)
- **Prediction Strategies**: MAX similarity + TOP-K (K=10,20,30,40,50)
- **Similarity Thresholds**: 0.6, 0.7, 0.8, 0.9, 1.0
- **Metrics**: TP, FP, Precision, Recall, F-Score, Prediction Rate

---

## 2. High-Level System Architecture

```mermaid
flowchart TB
    subgraph INPUT["Input Layer"]
        RAW["data/raw/Symptoms-Diagnosis.txt<br/>128 patient admissions<br/>Format: HADM_ID;SUBJECT_ID;...;SYMPTOMS;DIAGNOSIS"]
        FOLDS["data/folds/Fold0-Fold9/<br/>Pre-split 10-fold CV data<br/>TrainingSet.txt + TestSet.txt"]
    end

    subgraph ENTITY["Entity Layer (src/entity/)"]
        SD["SymptomsDiagnosis<br/>hadm_id, subject_id, admittime,<br/>dischtime, symptoms, diagnosis"]
        ADM["Admission<br/>19 MIMIC-III fields<br/>(row_id through has_chartevents_data)"]
        SYM["Symptom<br/>icd9_code, short_title"]
        DRG["Drgcodes<br/>drg_type, drg_code,<br/>severity, mortality"]
    end

    subgraph UTILS["Utilities Layer (src/utils/)"]
        CONST["Constants.py<br/>CH_DIR, K_FOLD=10, TOP_K,<br/>PRUNING_SIMILARITY=0.5"]
        CY["cython_utils.py<br/>cosine_similarity(), predictS2V(),<br/>load_dataset(), preprocess_sentence(),<br/>preprocess_diagnosis(), embeddings,<br/>confusion matrices, performance index"]
    end

    subgraph BASELINE["Baseline Pipeline (src/models/baseline_sent2vec.py)"]
        B_LOAD["Load BioSentVec Model<br/>BioSentVec_PubMed_MIMICIII-bigram_d700.bin<br/>~22.5 GB, 700 dimensions"]
        B_EMB["Compute Sent2Vec Embeddings<br/>embed_sentence() per symptom<br/>embed_sentence() per diagnosis"]
        B_PRED["10-Fold Prediction Loop<br/>predictS2V() per test case<br/>Similarity matrix (ntest x ntrain)"]
        B_PERF["Performance Computation<br/>Confusion matrices, P/R/F1<br/>Timing report + PDF graphs"]
    end

    subgraph BERT["BERT Pipeline (src/models/bert_models.py)"]
        M_SEL["Interactive Model Selection<br/>1: Bio_ClinicalBERT (MIMIC-III)<br/>2: BiomedBERT (PubMed)<br/>3: BlueBERT (PubMed+MIMIC)"]
        M_LOAD["Load SentenceTransformer<br/>768 dimensions<br/>Auto GPU/CPU detection"]
        M_EMB["Compute BERT Embeddings<br/>Batch encode (batch_size=32)<br/>Wrapped in arrays for compatibility"]
        M_PRED["10-Fold Prediction Loop<br/>Pure Python predict_topk_diagnoses_pure()<br/>No dependency on C-compiled predictS2V"]
        M_PERF["Performance Computation<br/>Same metrics + output format as baseline"]
    end

    subgraph EVAL["Evaluation (src/evaluation/bert_eval.py)"]
        EVAL_CLS["BioClinicalBERTEvaluator<br/>OOP alternative evaluator<br/>Direct HuggingFace AutoModel<br/>Mean pooling embeddings"]
    end

    subgraph SCRIPTS["Entry Points (scripts/)"]
        S1["run_baseline.py"]
        S2["run_bert_analysis.py"]
        S3["verify_setup.py"]
        S4["analyze_performance.py"]
    end

    subgraph OUTPUT["Output Layer"]
        PRED_DIR["Prediction_Output_{model}_{timestamp}/<br/>Fold0-9/ per-patient predictions<br/>PerformanceIndex.txt"]
        TIMING["timing_report.txt + timing_report.pdf"]
        PERF_PDF["performance_analysis.pdf<br/>(from analyze_performance.py)"]
    end

    RAW --> ENTITY
    FOLDS --> CY
    ENTITY --> BASELINE
    ENTITY --> BERT

    S1 -->|"import"| BASELINE
    S2 -->|"import"| BERT

    B_LOAD --> B_EMB --> B_PRED --> B_PERF
    M_SEL --> M_LOAD --> M_EMB --> M_PRED --> M_PERF

    CY -->|"preprocessing<br/>similarity<br/>evaluation"| BASELINE
    CY -->|"preprocessing<br/>evaluation<br/>fold loading"| BERT
    CONST --> CY

    BASELINE --> PRED_DIR
    BERT --> PRED_DIR
    B_PERF --> TIMING
    S4 --> PERF_PDF

    style INPUT fill:#e1f5fe,color:#000
    style ENTITY fill:#f3e5f5,color:#000
    style UTILS fill:#fff3e0,color:#000
    style BASELINE fill:#e8f5e9,color:#000
    style BERT fill:#fce4ec,color:#000
    style EVAL fill:#f1f8e9,color:#000
    style SCRIPTS fill:#ede7f6,color:#000
    style OUTPUT fill:#e0f2f1,color:#000
```

---

## 3. Data Flow Diagram -- Baseline Pipeline

```mermaid
flowchart LR
    subgraph LOAD["Phase 1: Data Loading"]
        F1["Symptoms-Diagnosis.txt"] -->|"read lines<br/>split by ';'"| PARSE["Parse into<br/>SymptomsDiagnosis objects"]
        PARSE -->|"preprocess_diagnosis()"| ADMISSIONS["admissions dict<br/>{HADM_ID: SymptomsDiagnosis}"]
    end

    subgraph MODEL["Phase 2: Model Loading"]
        BIN["BioSentVec_PubMed_MIMICIII<br/>-bigram_d700.bin<br/>(22.5 GB)"] -->|"sent2vec.Sent2vecModel()"| S2V_MODEL["Sent2Vec Model<br/>700-dim embeddings"]
    end

    subgraph EMBED["Phase 3: Embedding Computation"]
        S2V_MODEL --> EMB_S["embending_symptoms()<br/>For each admission:<br/>preprocess_sentence(symptom)<br/>model.embed_sentence()"]
        S2V_MODEL --> EMB_D["embending_diagnosis()<br/>For each admission:<br/>extract description after ':'<br/>model.embed_sentence()"]
        EMB_S --> SYM_DICT["embendings_symptoms dict<br/>{preprocessed_text: [embedding]}"]
        EMB_D --> DIAG_DICT["embendings_diagnosis dict<br/>{diagnosis_description: [embedding]}"]
    end

    subgraph CV["Phase 4: 10-Fold Cross-Validation"]
        direction TB
        FOLD["For nFold in 0..9"] --> LOAD_SPLIT["load_dataset(nFold, TRAIN)<br/>load_dataset(nFold, TEST)"]
        LOAD_SPLIT --> PREDICT["For each test case i:<br/>predictS2V()"]
        PREDICT --> SIM_MATRIX["Similarity Matrix<br/>(ntest x ntrain)<br/>cosine_similarity per<br/>symptom pair -> mean"]
    end

    subgraph PREDICTION["Phase 5: Prediction Strategies"]
        SIM_MATRIX --> MAX_SIM["MAX Similarity<br/>Find highest sim >= 0.5<br/>Get that patient's diagnosis"]
        SIM_MATRIX --> TOPK_SIM["TOP-K Similarity<br/>K=10,20,30,40,50<br/>Sort, take top-K >= 0.5"]
        MAX_SIM --> DIAG_SIM["get_diagnosis_similarity_by_description_max()<br/>cosine(GT_diag_emb, pred_diag_emb)"]
        TOPK_SIM --> DIAG_SIM
    end

    subgraph EVAL["Phase 6: Evaluation"]
        DIAG_SIM --> CM["Update Confusion Matrix<br/>For thresholds {0.6,0.7,0.8,0.9,1.0}:<br/>if diag_sim >= threshold: TP++<br/>else: FP++"]
        CM --> PERF["compute_performance_index()<br/>P = TP/(TP+FP)<br/>R = TP/nrow<br/>F = 2*P*R/(P+R)"]
        PERF --> AGG["Aggregate across folds<br/>print_performance_index()<br/>Mean over K_FOLD=10"]
    end

    ADMISSIONS --> EMBED
    ADMISSIONS --> CV
    SYM_DICT --> CV
    DIAG_DICT --> CV

    style LOAD fill:#e1f5fe,color:#000
    style MODEL fill:#fff3e0,color:#000
    style EMBED fill:#f3e5f5,color:#000
    style CV fill:#e8f5e9,color:#000
    style PREDICTION fill:#fce4ec,color:#000
    style EVAL fill:#ede7f6,color:#000
```

---

## 4. Data Flow Diagram -- BERT Pipeline

```mermaid
flowchart LR
    subgraph LOAD["Phase 1: Data Loading"]
        F1["Symptoms-Diagnosis.txt"] -->|"identical to baseline"| ADMISSIONS["admissions dict"]
    end

    subgraph SELECT["Phase 2: Model Selection"]
        MENU["Interactive CLI Menu"] --> BIO["Bio_ClinicalBERT<br/>emilyalsentzer/Bio_ClinicalBERT<br/>MIMIC-III notes"]
        MENU --> BIOMED["BiomedBERT<br/>microsoft/BiomedNLP-BiomedBERT<br/>PubMed abstracts"]
        MENU --> BLUE["BlueBERT<br/>bionlp/bluebert_pubmed_mimic<br/>PubMed + MIMIC-III"]
    end

    subgraph MODEL["Phase 3: Model Loading"]
        BIO --> ST["SentenceTransformer(model_path)<br/>768 dimensions<br/>Auto GPU/CPU"]
        BIOMED --> ST
        BLUE --> ST
    end

    subgraph EMBED["Phase 4: BERT Embedding Computation"]
        ST --> BERT_SYM["compute_bert_symptom_embeddings()<br/>preprocess_sentence(admission.symptoms)<br/>model.encode(batch_size=32)<br/>Wrap in [embedding] arrays"]
        ST --> BERT_DIAG["compute_bert_diagnosis_embeddings()<br/>Collect unique diagnosis texts<br/>model.encode(batch_size=32)<br/>Key by diagnosis TEXT not patient ID"]
    end

    subgraph CV["Phase 5: 10-Fold CV (Pure Python)"]
        direction TB
        FOLD["For nFold in 0..9"] --> LOAD_SPLIT["util_cy.load_dataset()"]
        LOAD_SPLIT --> PREDICT["predict_topk_diagnoses_pure()<br/>Compute symptom similarity<br/>Sort by similarity desc<br/>No PRUNING_SIMILARITY threshold"]
    end

    subgraph EVAL["Phase 6: Evaluation"]
        PREDICT --> MAX["MAX Strategy (k=None)<br/>Single best match"]
        PREDICT --> TOPK["TOP-K Strategy (k=10..50)"]
        MAX --> DIAG_SIM["util_cy.get_diagnosis_similarity_by_description_max()<br/>Reuses baseline function exactly"]
        TOPK --> DIAG_SIM
        DIAG_SIM --> METRICS["Same confusion matrix + perf index<br/>as baseline (util_cy functions)"]
    end

    ADMISSIONS --> EMBED
    ADMISSIONS --> CV

    style LOAD fill:#e1f5fe,color:#000
    style SELECT fill:#fff9c4,color:#000
    style MODEL fill:#fff3e0,color:#000
    style EMBED fill:#f3e5f5,color:#000
    style CV fill:#e8f5e9,color:#000
    style EVAL fill:#ede7f6,color:#000
```

---

## 5. Component Relationship Diagram

```mermaid
classDiagram
    direction TB

    class SymptomsDiagnosis {
        +str hadm_id
        +str subject_id
        +str admittime
        +str dischtime
        +str symptoms
        +list diagnosis
        +CONST_HADM_ID = 0
        +CONST_SUBJECT_ID = 1
        +CONST_SYMPTOMS = 4
        +CONST_DIAGNOSIS = 5
    }

    class Admission {
        +str row_id
        +str subject_id
        +str hadm_id
        +str admittime
        +str diagnosis
        +str hospital_expire_flag
        <<19 MIMIC-III fields>>
    }

    class Symptom {
        +str icd9_code
        +str short_title
    }

    class Drgcodes {
        +str hadm_id
        +str drg_type
        +str drg_code
        +str description
        +str drg_severity
        +str drg_mortality
    }

    class Constants {
        +str CH_DIR
        +str FMT
        +int K_FOLD = 10
        +float PRUNING_SIMILARITY = 0.5
        +int TOP_K_LOWER_BOUND = 10
        +int TOP_K_UPPER_BOUND = 60
        +int TOP_K_INCR = 10
    }

    class cython_utils {
        +cosine_similarity(u, v)
        +predictS2V(...)
        +load_dataset(nFold, name)
        +embending_symptoms(model, admissions)
        +embending_diagnosis(model, admissions)
        +load_model()
        +preprocess_sentence(text)
        +preprocess_diagnosis(diagnosis)
        +init_confusion_matrix()
        +init_performance_matrix()
        +compute_performance_index(...)
        +compute_aggregated_performance_index(...)
        +print_performance_index(...)
    }

    class baseline_sent2vec {
        <<module-level execution>>
        +ensure_nltk_data()
        +format_time(seconds)
        +generate_timing_pdf(timing_data, path)
        Dataset loading
        Model loading (BioSentVec)
        Embedding computation
        10-fold CV loop
        Performance aggregation
    }

    class bert_models {
        <<module-level execution>>
        +MODELS dict
        +select_model()
        +compute_patient_similarity_pure(...)
        +predict_topk_diagnoses_pure(...)
        +compute_bert_symptom_embeddings(model, admissions)
        +compute_bert_diagnosis_embeddings(model, admissions)
        Dataset loading
        Model loading (SentenceTransformer)
        10-fold CV loop
    }

    class BioClinicalBERTEvaluator {
        +str model_name
        +tokenizer
        +model
        +device
        +admissions dict
        +embeddings_symptoms dict
        +embeddings_diagnosis dict
        +load_model()
        +load_dataset()
        +get_bert_embedding(text)
        +compute_symptom_embeddings()
        +compute_diagnosis_embeddings()
        +predict_topk_diagnoses(...)
        +evaluate_prediction(...)
        +run_evaluation()
    }

    baseline_sent2vec --> cython_utils : uses all functions
    baseline_sent2vec --> SymptomsDiagnosis : creates instances
    baseline_sent2vec --> Constants : imports config

    bert_models --> cython_utils : preprocessing + evaluation
    bert_models --> SymptomsDiagnosis : creates instances
    bert_models --> Constants : imports config

    BioClinicalBERTEvaluator --> cython_utils : preprocessing only
    BioClinicalBERTEvaluator --> SymptomsDiagnosis : creates instances
    BioClinicalBERTEvaluator --> Constants : imports config

    cython_utils --> Constants : imports CH_DIR, params
    cython_utils --> SymptomsDiagnosis : loads into
```

---

## 6. Prediction Algorithm Sequence Diagram

```mermaid
sequenceDiagram
    participant Script as run_baseline.py / run_bert_analysis.py
    participant Model as baseline_sent2vec / bert_models
    participant Utils as cython_utils
    participant Data as data/folds/

    Script->>Model: import (triggers execution)
    Model->>Utils: load_dataset(file_name) [Symptoms-Diagnosis.txt]
    Utils-->>Model: admissions dict {HADM_ID: SymptomsDiagnosis}

    alt Baseline
        Model->>Utils: load_model() [BioSentVec 22.5GB]
        Utils-->>Model: sent2vec model
        Model->>Utils: embending_symptoms(model, admissions)
        Utils-->>Model: symptom embeddings dict
        Model->>Utils: embending_diagnosis(model, admissions)
        Utils-->>Model: diagnosis embeddings dict
    else BERT
        Model->>Model: select_model() [interactive menu]
        Model->>Model: SentenceTransformer(model_path) [HuggingFace]
        Model->>Model: compute_bert_symptom_embeddings()
        Model->>Model: compute_bert_diagnosis_embeddings()
    end

    loop For each fold (0-9)
        Model->>Data: load_dataset(nFold, TRAIN)
        Data-->>Model: x_train [{HADM_ID: [symptoms]}]
        Model->>Data: load_dataset(nFold, TEST)
        Data-->>Model: x_test [{HADM_ID: [symptoms]}]

        loop For each test case
            alt Baseline
                Model->>Utils: predictS2V(test, train, embeddings, ...)
                Note over Utils: Compute symptom similarity matrix<br/>MAX + TOP-K prediction<br/>Diagnosis similarity evaluation<br/>Update confusion matrices
                Utils-->>Model: Updated confusion matrices
            else BERT
                Model->>Model: predict_topk_diagnoses_pure(test, train, emb, k)
                Model->>Utils: get_diagnosis_similarity_by_description_max(...)
                Utils-->>Model: diagnosis similarity score
                Model->>Model: Update confusion matrices at thresholds
            end
        end

        Model->>Utils: compute_aggregated_performance_index(...)
        Utils-->>Model: P, R, F-Score per threshold
    end

    Model->>Utils: print_performance_index(performance_matrix)
    Model->>Model: Write PerformanceIndex.txt + timing reports
```

---

## 7. Module Dependency Graph

```mermaid
flowchart TD
    subgraph External["External Dependencies"]
        SENT2VEC["sent2vec<br/>(BioSentVec model)"]
        SENTENCE_TR["sentence-transformers<br/>(HuggingFace)"]
        TRANSFORMERS["transformers<br/>(AutoModel, AutoTokenizer)"]
        TORCH["torch<br/>(PyTorch)"]
        GENSIM["gensim<br/>(KeyedVectors)"]
        NLTK["nltk<br/>(stopwords, punkt)"]
        SCIPY["scipy<br/>(spatial.distance)"]
        SKLEARN["scikit-learn<br/>(KFold)"]
        NUMPY["numpy"]
        MATPLOTLIB["matplotlib<br/>(PDF reports)"]
    end

    subgraph Internal["Project Modules"]
        CONST["src/utils/Constants"]
        CY["src/utils/cython_utils"]
        SD["src/entity/SymptomsDiagnosis"]
        ADM_E["src/entity/Admission"]
        SYM_E["src/entity/Symptom"]
        DRG_E["src/entity/Drgcodes"]
        BL["src/models/baseline_sent2vec"]
        BM["src/models/bert_models"]
        BE["src/evaluation/bert_eval"]
    end

    BL --> CY
    BL --> SD
    BL --> CONST
    BL --> SENT2VEC
    BL --> GENSIM
    BL --> NLTK
    BL --> NUMPY
    BL --> SCIPY
    BL --> MATPLOTLIB

    BM --> CY
    BM --> SD
    BM --> CONST
    BM --> SENTENCE_TR
    BM --> NLTK
    BM --> NUMPY
    BM --> SCIPY
    BM --> MATPLOTLIB

    BE --> CY
    BE --> SD
    BE --> CONST
    BE --> TRANSFORMERS
    BE --> TORCH
    BE --> NLTK
    BE --> NUMPY
    BE --> SCIPY

    CY --> CONST
    CY --> GENSIM
    CY --> SCIPY
    CY --> SKLEARN
    CY --> NUMPY
    CY --> NLTK
    CY --> SENT2VEC

    style External fill:#f5f5f5,color:#000
    style Internal fill:#e8f5e9,color:#000
```

---

## 8. Data Schema

### 8.1 Raw Data Format (Symptoms-Diagnosis.txt)
```
HADM_ID ; SUBJECT_ID ; ADMIT_TIME ; DISCHARGE_TIME ; SYMPTOMS ; DIAGNOSIS
142345  ; 10006      ; 2164-10-23 ; 2164-11-01     ; Sepsis,...| HCFA:SEPTICEMIA AGE >17
```
- **128 records** total
- Symptoms: comma-separated ICD-9 short titles
- Diagnosis: DRG type prefix (HCFA/APR/MS) + colon + description, separated by "--"

### 8.2 Fold Data Format (TrainingSet.txt / TestSet.txt)
```
HADM_ID_Symptom1,Symptom2,Symptom3,...
142345_Sepsis,React-oth vasc dev/graft,...
```
- Training: ~116 records per fold
- Test: ~13 records per fold

### 8.3 Output Format (PerformanceIndex.txt)
```
 FOLD 0: LEN train: 116, LEN test: 13
 0 - HADM_ID=124073: PERFORMANCE INDEX of MAX SIMILARITY by MAX
     TP    FP    P    R    FS    PR
 1.0  ...
 0.9  ...
 ...
 PERFORMANCE INDEX of TOP-10 SIMILARITY by MAX
 ...
 ****
 10-FOLD PERFORMANCE INDEX of MAX SIMILARITY by MAX
 [Aggregated means across all folds]
```

---

## 9. Key Architectural Decisions and Observations

### 9.1 Design Pattern: Module-Level Execution
Both `baseline_sent2vec.py` and `bert_models.py` execute their entire pipeline at **import
time**. The entry point scripts (`run_baseline.py`, `run_bert_analysis.py`) simply import
the module, which triggers execution. This is a direct port from the original single-file
`CS2V.py` script.

### 9.2 Shared Core via cython_utils
The `cython_utils.py` module is the shared backbone. Originally a compiled Cython `.c` file
(`util_cy.c` in the archive), it has been ported to pure Python. Both pipelines reuse:
- `preprocess_sentence()` and `preprocess_diagnosis()` for text normalization
- `load_dataset()` for fold loading
- `init_confusion_matrix()` and `init_performance_matrix()`
- `compute_performance_index()` and `compute_aggregated_performance_index()`
- `cosine_similarity()` for vector comparison
- `get_diagnosis_similarity_by_description_max()` for diagnosis evaluation

### 9.3 Embedding Compatibility Layer
BERT embeddings are wrapped in single-element arrays (`[embedding]`) to match the format
expected by `cython_utils.py` functions that index with `[0]`. This is a pragmatic
compatibility shim.

### 9.4 Two BERT Implementations
There are TWO BERT evaluation approaches:
1. **bert_models.py** (primary): Uses `SentenceTransformer` from sentence-transformers lib,
   pure Python prediction, reuses `util_cy` evaluation functions
2. **bert_eval.py** (alternative): Uses `AutoModel`/`AutoTokenizer` directly from
   transformers, has its own OOP evaluator class, standalone evaluation logic

### 9.5 No PRUNING_SIMILARITY in BERT Pipeline
The BERT pipeline's `predict_topk_diagnoses_pure()` does NOT apply the
`PRUNING_SIMILARITY=0.5` threshold during prediction (unlike the baseline's `predictS2V`).
This could lead to different behavior in edge cases.

### 9.6 Cython Legacy
The original system used a compiled Cython module (`util_cy.c` / `util_cy.pyd`). The
archive directory preserves the original C source. The current `cython_utils.py` is a
pure Python reimplementation that eliminated the Cython compilation requirement
(which had Python version compatibility issues with 3.10+).

### 9.7 Large Model Dependency
The baseline requires a 22.5 GB BioSentVec model file that must be manually downloaded.
BERT models are downloaded automatically from HuggingFace (~500MB each).

### 9.8 Academic Context
This is a Georgia Tech CSE 6250 (Big Data for Health Informatics) course project.
The `docs/Reproduce_w_transformers.md` document contains SLURM cluster deployment
scripts and a full IEEE-format report outline, indicating this is being developed
for academic submission.

---

## 10. Testing Architecture

```mermaid
flowchart TD
    subgraph Tests["tests/"]
        TS["test_setup.py<br/>Smoke test: imports,<br/>paths, dependencies,<br/>NLTK data, fold files"]
        TBI["test_bert_integration.py<br/>BERT model load test,<br/>embedding generation,<br/>similarity computation"]
        TR["test_reorganization.py<br/>Pytest classes:<br/>TestImports, TestDataPaths,<br/>TestDirectoryStructure,<br/>TestConfigFiles,<br/>TestConstantsPathResolution,<br/>TestEntityClasses"]
    end

    subgraph Scripts["scripts/"]
        VS["verify_setup.py<br/>Directory structure check,<br/>import validation,<br/>data file verification,<br/>config file check,<br/>CH_DIR resolution"]
    end

    TS -->|"verifies"| ENTITY["Entity imports"]
    TS -->|"verifies"| CONST["Constants.CH_DIR"]
    TS -->|"verifies"| FOLDS["Fold data files"]
    TS -->|"verifies"| DEPS["Python dependencies"]

    TBI -->|"verifies"| BERT_LOAD["BERT model loading"]
    TBI -->|"verifies"| BERT_EMB["Embedding generation"]
    TBI -->|"verifies"| BERT_SIM["Similarity computation"]

    TR -->|"pytest"| IMPORTS["Module imports"]
    TR -->|"pytest"| PATHS["Data paths"]
    TR -->|"pytest"| DIRS["Directory structure"]
    TR -->|"pytest"| CONFIG["Config files"]
    TR -->|"pytest"| ENTITIES["Entity instantiation"]

    VS -->|"verifies"| DIR_STRUCT["Full directory tree"]
    VS -->|"verifies"| IMPORTS
    VS -->|"verifies"| FOLDS
    VS -->|"verifies"| CONFIG
    VS -->|"verifies"| CONST

    style Tests fill:#fff3e0,color:#000
    style Scripts fill:#e1f5fe,color:#000
```

---

## 11. Output and Reporting Pipeline

```mermaid
flowchart LR
    subgraph Execution["Model Execution"]
        BL["Baseline Run"] --> PO_BL["Prediction_Output_{timestamp}/"]
        BERT["BERT Run"] --> PO_BERT["Prediction_Output_{model}_{timestamp}/"]
    end

    subgraph PerPrediction["Per-Fold Outputs"]
        PO_BL --> FOLD_DIR["Fold0/ ... Fold9/<br/>Per-patient prediction files"]
        PO_BERT --> FOLD_DIR
    end

    subgraph PerformanceFiles["Performance Files"]
        PO_BL --> PI["PerformanceIndex.txt<br/>Per-patient metrics<br/>Per-fold aggregates<br/>10-fold mean"]
        PO_BERT --> PI
        PO_BL --> TR_TXT["timing_report.txt"]
        PO_BL --> TR_PDF["timing_report.pdf<br/>(matplotlib graphs)"]
    end

    subgraph Analysis["Post-Run Analysis"]
        PI --> AP["scripts/analyze_performance.py"]
        AP --> PA_PDF["performance_analysis.pdf<br/>- Aggregate bar charts<br/>- Threshold sensitivity<br/>- PR curves<br/>- Fold variance<br/>- F-Score heatmap<br/>- TOP-K comparison"]
    end

    style Execution fill:#e8f5e9,color:#000
    style PerPrediction fill:#e1f5fe,color:#000
    style PerformanceFiles fill:#f3e5f5,color:#000
    style Analysis fill:#fff3e0,color:#000
```

---

## 12. Deployment Architecture

```mermaid
flowchart TB
    subgraph Local["Local Development"]
        DEV["Developer Machine<br/>Python 3.9+<br/>pip/conda"]
        DEV --> BASELINE["Baseline: needs BioSentVec<br/>22.5 GB model + sent2vec<br/>Cython compilation"]
        DEV --> BERT_LOCAL["BERT: auto-downloads<br/>from HuggingFace<br/>~500MB per model"]
    end

    subgraph Cluster["Georgia Tech SLURM Cluster"]
        SLURM["SBATCH Job<br/>gpu-h200 partition<br/>1x H200 GPU, 128GB RAM"]
        SLURM --> CUDA["CUDA 12.1 + PyTorch"]
        CUDA --> BERT_GPU["BERT w/ GPU acceleration<br/>~2GB GPU memory per model<br/>2-3 hours per 10-fold CV"]
    end

    subgraph Config["Configuration"]
        ENV["config/environment.yml<br/>(conda)"]
        REQ["config/requirements.txt<br/>(baseline pip)"]
        REQ_BERT["config/requirements_bert.txt<br/>(BERT pip)"]
    end

    Config --> Local
    Config --> Cluster

    style Local fill:#e1f5fe,color:#000
    style Cluster fill:#fff3e0,color:#000
    style Config fill:#e8f5e9,color:#000
```
